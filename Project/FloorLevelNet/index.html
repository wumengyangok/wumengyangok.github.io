<html class="gr__richzhang_github_io"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./The Unreasonable Effectiveness of Deep Networks as a Perceptual Metric_files/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	h1 {
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 0px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style> 
		<title>FloorLevel-Net: Recognizing Floor-Level Lines with Height-Attention-Guided Multi-task Learning</title>
		<!-- <meta property="og:image" content="https://yulequan.github.io//ec-net/figures/teaser_one_column_high.png">
		<meta property="og:title" content="PU-GAN: a Point Cloud Upsampling Adversarial Network. In ICCV, 2019."> -->
  </head>

  <body data-gr-c-s-loaded="true">
    <br>
          <center>
          	<span style="font-size:34px">FloorLevel-Net: Recognizing Floor-Level Lines with Height-Attention-Guided Multi-task Learning</span><br>
	  		  
	  		  <br>

	  		  <table align="center" width="900px">
	  			  <tbody><tr>
	  	              <td align="center" width="150px">
	  					<center>
	  						<span style="font-size:22px"><a href="https://wumengyangok.github.io/" target="_blank" >Mengyang Wu</a><sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="150px">
	  					<center>
	  						<span style="font-size:22px"><a href="https://zeng-wei.com/"  target="_blank" >Wei Zeng</a><sup>2</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="200px">
	  					<center>
	  						<span style="font-size:22px"><a href="http://www.cse.cuhk.edu.hk/~cwfu/" target="_blank" >Chi-Wing Fu</a><sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  			  </tr>
			  </tbody></table>
	  		  
			<!--   <table align="center" width="800px"><tbody>
			  <tr>
	  	              <td align="center" width="50px"></td>
	  	              <td align="center" width="400px">
	  					<center>
				          	<span style="font-size:18px"><sup>1</sup>The Chinese University of Hong Kong</span>
		  		  		</center>
		  	      </td>
	  	              <td align="center" width="300px">
	  					<center>
				          	<span style="font-size:18px"><sup>2</sup>Tel Aviv University</span>
		  		  		</center>
		  	      </td>
	  	              <td align="center" width="50px"></td>
			  </tr>
			  </tbody></table> -->

			  <table align="center" width="800px"><tbody>
			  <tr>
	  	              <td align="center" width=800px">
	  					<center>
							  <span style="font-size:18px"><sup>1</sup>
								The Chinese University of Hong Kong, Hong Kong, China
							</span>
		  		  		</center>
		  	      </td>
			  </tr>
			  </tbody></table>
			  
			  <table align="center" width="800px"><tbody>
			  <tr>
	  	              <td align="center" width=800px">
	  					<center>
							  <span style="font-size:18px"><sup>2</sup>
								Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, China
							</span>
		  		  		</center>
		  	      </td>
			  </tr>
			  </tbody></table>

			  <br>

	  		  <table align="center" width="1100px">
	  			  <tbody><tr>
	  	              <td align="center" width="275px">
	  					<center>
				          	<span style="font-size:18px"></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="225px">
	  					<center>
	  						<span style="font-size:22px">Code + Dataset<a href="https://github.com/wumengyangok/FloorLevelNet" target="_blank" > [GitHub]</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="225px">
	  					<center>
							  <span style="font-size:22px">Paper<a href="" target="_blank" > [Paper]</a></span>
	  						  <!-- <span style="font-size:22px">ICCV 2019 [Paper]</span> -->
							  
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="275px">
	  					<center>
				          	<span style="font-size:18px"></span>
		  		  		</center>
		  		  	  </td>
			  </tr></tbody></table>
          </center>

          <br>
  		  <table align="center" width="1100px">
  			  <tbody><tr>
  	              <td width="400px">
  					<center>
  	                	<img class="rounded" src="./figures/teaser.png" width="800px">
  	                	<br>
				 
					</center>
					<br>
<strong>Left</strong> column shows two example street-view images in London (top)
and Hong Kong (bottom), where the camera views are side- and front-facing
relative to the building, respectively. Note the occlusions introduced by the
advertisement billboard and light post circled in red on bottom left. <strong>Middle</strong>
column shows floor-level lines recognized by our method with geometric
positions and semantic order labels. <strong>Right</strong> column shows potential floor-aware
image-overlay results to aid shopping (top) and navigation (bottom).
  	              </td>
  	              </tr>
  	              </tbody></table>

  		  <br>
		  <hr>

  		  <center><h1>Abstract</h1></center>
			The ability to recognize the position and order of the
floor-level lines that divide adjacent building floors can benefit
many applications, for example, urban augmented reality (AR).
This work tackles the problem of locating floor-level lines in
street-view images, using a supervised deep learning approach.
Unfortunately, very little data is available for training such a
network âˆ’ current street-view datasets contain either semantic
annotations that lack geometric attributes, or rectified facades
without perspective priors. To address this issue, we first
compile a new dataset and develop a new data augmentation
scheme to synthesize training samples by harassing (i) the
rich semantics of existing rectified facades and (ii) perspective
priors of buildings in diverse street views. Next, we design
FloorLevel-Net, a multi-task learning network that associates
explicit features of building facades and implicit floor-level lines,
along with a height-attention mechanism to help enforce a vertical
ordering of floor-level lines. The generated segmentations are
then passed to a second-stage geometry post-processing to exploit
self-constrained geometric priors for plausible and consistent
reconstruction of floor-level lines. Quantitative and qualitative
evaluations conducted on assorted facades in existing datasets
and street views from Google demonstrate the effectiveness of
our approach. Also, we present context-aware image overlay
results and show the potentials of our approach in enriching AR-related applications.
		<br><br><hr>

		<center><h1>Overview</h1></center>
		Our two-stage approach: (i) FloorLevel-Net is a multi-task learning network that segments the input image into building-facade-wise semantic regions
		(top) and floor-level distributions (bottom); and (ii) our method further fits and refines the pixel-wise network outputs into polylines with geometric parameters.
		Further, we can take the reconstructed floor-level lines to support and enrich urban AR applications with floor-aware image overlay.
  		    <table align="center" width="900px">
  			<tbody>
  			  	<tr>
  			  		<td align="center"><img class="round" style="width:1000px" src="./figures/overview.png"></td>	
			  	</tr>
			  	<!-- <tr>
			  		<td align="center"><h3>Pipeline of EC-Net</h3></td>
			  	</tr> -->
			</tbody>
			</table>
			
			<br>

			<!-- <table align="center" width="900px">
  			<tbody>
  			  	<tr>
  			  		<td align="center"><img class="round" style="width:700px" src="./figures/up&down.png"></td>	
			  	</tr>
			  	<tr>
			  		<td align="center"><h3>Patches Extraction</h3></td>
			  	</tr>

			</tbody>
			</table> -->
		 

  		  <!-- <center>
				<span style="font-size:28px"><a href="https://github.com/liruihui/PU-GAN">[GitHub]</a>
			  <br>
			  </span></center>
		  <br> -->

		  <hr>

		  <center><h1>Qualitative comparison results<br></h1></center>
		 The results illustrate the effects of (i) our data augmentation
		 scheme by comparing DeeplabV3+ models trained on CMP and on our training data (green background), (ii) multi-task learning with and without the height-
	     attention mechanism (red background), and (iii) our full method further with geometry post-processing (yellow background), vs. the ground truths (GT).
  		<br><br>
  		<table align="center" width="600px">
			  <tbody><tr>
				  <td><span style="font-size:24pt"><center>
				  	<img class="paper-big" style="width:1000px" src="./figures/result_1.png">
  	              </center></span></td>
              </tr>
  		</tbody></table> 
  		
  		<!-- <center>
  			<br>
  			<span style="font-size:28px"><a href="./moreresult.html">[More results]</a>
			<br>
			</span>
		</center> -->


 		<center><h1>AR overlaying results<br></h1></center>
		The potential of our approach to support and enrich various AR scenarios, e.g., navigation, advertisement, etc.
		<br><br>
  	
  		<table align="center" width="600px">
			  <tbody><tr>
				  <td><span style="font-size:24pt"><center>
				  	<img class="paper-big" style="width:1000px" src="./figures/result_ar.png">
  	              </center></span></td>
              </tr>
  		</tbody></table> 

  		<br>
		  <br>
		  
		<hr>

	

 

		

<!-- Global site tag (gtag.js) - Google Analytics -->



</body></html>
